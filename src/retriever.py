"""
Advanced retrieval strategies for the RAG Financial Filing Analyst.

Provides:
- **Multi-query retrieval** — expands the user query into multiple variants
  to improve recall.
- **Contextual compression** — reranks and compresses retrieved chunks
  to retain only the most query-relevant passages.
- **Metadata-filtered retrieval** — restricts results by company, section,
  and filing date range.
- **Maximum marginal relevance (MMR)** — diversifies the result set to
  reduce redundancy.
- **Ensemble retriever** — combines dense, sparse, and MMR strategies via
  reciprocal rank fusion.
"""

from __future__ import annotations

import logging
from dataclasses import dataclass, field
from typing import Any, Optional

import numpy as np

from src.config import RetrievalStrategy, Settings
from src.embeddings import EmbeddingPipeline
from src.vector_store import SearchResult, VectorStore

logger = logging.getLogger(__name__)

# ---------------------------------------------------------------------------
# Data classes
# ---------------------------------------------------------------------------


@dataclass
class RetrievalResult:
    """Aggregated retrieval output from any strategy.

    Attributes:
        results: Ranked list of search results.
        query: The original user query.
        expanded_queries: Any additional queries generated by multi-query expansion.
        strategy: Name of the retrieval strategy that produced these results.
    """

    results: list[SearchResult] = field(default_factory=list)
    query: str = ""
    expanded_queries: list[str] = field(default_factory=list)
    strategy: str = ""

    @property
    def texts(self) -> list[str]:
        """Return the text content of all results."""
        return [r.text for r in self.results]

    @property
    def scores(self) -> list[float]:
        """Return the scores of all results."""
        return [r.score for r in self.results]


# ---------------------------------------------------------------------------
# Query expansion
# ---------------------------------------------------------------------------


class QueryExpander:
    """Generate multiple query variants from a single user question.

    Uses an LLM to rephrase the query from different perspectives, improving
    the chance that at least one variant matches the relevant chunk
    embeddings closely.

    Args:
        llm_fn: A callable that takes a prompt string and returns a string.
        num_variants: Number of alternative queries to generate.
    """

    DEFAULT_EXPANSION_PROMPT = (
        "You are a financial analyst research assistant. Your task is to "
        "generate {n} different versions of the following user question "
        "to help retrieve relevant information from SEC 10-K filings. "
        "Each version should approach the question from a slightly "
        "different angle while preserving the original intent.\n\n"
        "Original question: {query}\n\n"
        "Provide {n} alternative questions, one per line. Do not number them."
    )

    def __init__(
        self,
        llm_fn: Any = None,
        num_variants: int = 3,
    ) -> None:
        self._llm_fn = llm_fn
        self._num_variants = num_variants

    def expand(self, query: str) -> list[str]:
        """Generate query variants.

        If no LLM function is available, falls back to simple heuristic
        expansions (synonym substitution, perspective shift).

        Args:
            query: The original user query.

        Returns:
            A list containing the original query plus generated variants.
        """
        variants = [query]

        if self._llm_fn is not None:
            try:
                prompt = self.DEFAULT_EXPANSION_PROMPT.format(
                    n=self._num_variants, query=query
                )
                response = self._llm_fn(prompt)
                generated = [
                    line.strip()
                    for line in response.strip().split("\n")
                    if line.strip()
                ]
                variants.extend(generated[: self._num_variants])
                logger.info(
                    "Expanded query into %d variants (LLM)", len(variants)
                )
                return variants
            except Exception as exc:
                logger.warning("LLM query expansion failed: %s; using fallback", exc)

        # Heuristic fallback expansions
        variants.append(f"What does the 10-K filing say about: {query}")
        variants.append(f"financial data related to {query}")
        variants.append(f"SEC filing information: {query}")
        logger.info("Expanded query into %d variants (heuristic)", len(variants))
        return variants


# ---------------------------------------------------------------------------
# Contextual compression / reranking
# ---------------------------------------------------------------------------


class ContextualCompressor:
    """Rerank and optionally compress retrieved chunks for query relevance.

    Scores each chunk against the query using embedding cosine similarity
    and keeps only the top-N most relevant.  Optionally extracts the most
    relevant sentences from each chunk (compression).

    Args:
        embedding_pipeline: Used for scoring chunks against the query.
        top_n: Number of chunks to retain after reranking.
        compress: If ``True``, extract the most relevant sentences per chunk.
    """

    def __init__(
        self,
        embedding_pipeline: EmbeddingPipeline,
        top_n: int = 5,
        compress: bool = False,
    ) -> None:
        self._embedder = embedding_pipeline
        self._top_n = top_n
        self._compress = compress

    def rerank(
        self, query: str, results: list[SearchResult]
    ) -> list[SearchResult]:
        """Rerank results by cosine similarity to the query embedding.

        Args:
            query: The user query.
            results: Candidate search results.

        Returns:
            Reranked (and possibly truncated) list of search results.
        """
        if not results:
            return results

        query_emb = self._embedder.embed_query(query)
        texts = [r.text for r in results]
        chunk_embs = self._embedder.embed_texts(
            texts, use_cache=True, show_progress=False
        )

        # Cosine similarity
        norms_q = np.linalg.norm(query_emb)
        norms_c = np.linalg.norm(chunk_embs, axis=1)
        similarities = chunk_embs @ query_emb / (norms_c * norms_q + 1e-10)

        # Sort by reranked score
        ranked_indices = np.argsort(-similarities)
        reranked: list[SearchResult] = []
        for idx in ranked_indices[: self._top_n]:
            result = results[idx]
            result.score = float(similarities[idx])
            if self._compress:
                result.text = self._extract_relevant_sentences(
                    query_emb, result.text
                )
            reranked.append(result)

        logger.info(
            "Reranked %d -> %d results", len(results), len(reranked)
        )
        return reranked

    def _extract_relevant_sentences(
        self, query_emb: np.ndarray, text: str, top_sentences: int = 3
    ) -> str:
        """Extract the most relevant sentences from a chunk.

        Args:
            query_emb: Query embedding vector.
            text: Full chunk text.
            top_sentences: Number of sentences to keep.

        Returns:
            Compressed text containing only the most relevant sentences.
        """
        import re

        sentences = re.split(r"(?<=[.!?])\s+", text)
        if len(sentences) <= top_sentences:
            return text

        sent_embs = self._embedder.embed_texts(
            sentences, use_cache=False, show_progress=False
        )
        norms_q = np.linalg.norm(query_emb)
        norms_s = np.linalg.norm(sent_embs, axis=1)
        sims = sent_embs @ query_emb / (norms_s * norms_q + 1e-10)

        top_indices = np.argsort(-sims)[:top_sentences]
        # Preserve original sentence order
        top_indices = sorted(top_indices)
        return " ".join(sentences[i] for i in top_indices)


# ---------------------------------------------------------------------------
# MMR diversifier
# ---------------------------------------------------------------------------


def mmr_diversify(
    query_embedding: np.ndarray,
    results: list[SearchResult],
    chunk_embeddings: np.ndarray,
    top_k: int = 5,
    lambda_param: float = 0.7,
) -> list[SearchResult]:
    """Select results using Maximum Marginal Relevance.

    Balances relevance to the query with diversity among selected results.

    Args:
        query_embedding: 1-D query vector.
        results: Candidate search results.
        chunk_embeddings: Embeddings corresponding to *results* (same order).
        top_k: Number of results to select.
        lambda_param: Trade-off between relevance (1.0) and diversity (0.0).

    Returns:
        A list of ``top_k`` diversified ``SearchResult`` objects.
    """
    if len(results) <= top_k:
        return results

    # Precompute similarities
    query_norm = np.linalg.norm(query_embedding)
    chunk_norms = np.linalg.norm(chunk_embeddings, axis=1, keepdims=True)
    query_sims = (chunk_embeddings @ query_embedding) / (
        chunk_norms.flatten() * query_norm + 1e-10
    )

    # Pairwise chunk similarities
    normed = chunk_embeddings / (chunk_norms + 1e-10)
    pairwise = normed @ normed.T

    selected_indices: list[int] = []
    remaining = set(range(len(results)))

    for _ in range(top_k):
        best_idx = -1
        best_score = -float("inf")

        for idx in remaining:
            relevance = query_sims[idx]
            if selected_indices:
                max_sim_to_selected = max(
                    pairwise[idx][s] for s in selected_indices
                )
            else:
                max_sim_to_selected = 0.0

            mmr_score = lambda_param * relevance - (1 - lambda_param) * max_sim_to_selected
            if mmr_score > best_score:
                best_score = mmr_score
                best_idx = idx

        if best_idx >= 0:
            selected_indices.append(best_idx)
            remaining.discard(best_idx)

    mmr_results = []
    for idx in selected_indices:
        result = results[idx]
        result.score = float(query_sims[idx])
        mmr_results.append(result)

    logger.info(
        "MMR diversification: %d -> %d results (lambda=%.2f)",
        len(results),
        len(mmr_results),
        lambda_param,
    )
    return mmr_results


# ---------------------------------------------------------------------------
# Ensemble retriever
# ---------------------------------------------------------------------------


class EnsembleRetriever:
    """Combines multiple retrieval strategies via reciprocal rank fusion.

    Runs dense similarity search, BM25 sparse search, and (optionally)
    MMR-diversified retrieval in parallel, then fuses results.

    Args:
        vector_store: The vector store instance.
        embedding_pipeline: Embedding pipeline for query encoding.
        settings: Application configuration.
        compressor: Optional contextual compressor for post-retrieval reranking.
        query_expander: Optional query expander for multi-query retrieval.
    """

    def __init__(
        self,
        vector_store: VectorStore,
        embedding_pipeline: EmbeddingPipeline,
        settings: Settings,
        compressor: Optional[ContextualCompressor] = None,
        query_expander: Optional[QueryExpander] = None,
    ) -> None:
        self._store = vector_store
        self._embedder = embedding_pipeline
        self._settings = settings
        self._compressor = compressor
        self._expander = query_expander

    def retrieve(
        self,
        query: str,
        top_k: Optional[int] = None,
        metadata_filter: Optional[dict[str, Any]] = None,
        strategy: Optional[RetrievalStrategy] = None,
    ) -> RetrievalResult:
        """Execute retrieval with the specified (or default) strategy.

        Args:
            query: User query string.
            top_k: Override number of results.
            metadata_filter: Optional metadata constraints.
            strategy: Override the default strategy from config.

        Returns:
            A ``RetrievalResult`` with ranked search results.
        """
        k = top_k or self._settings.retrieval_top_k
        strat = strategy or self._settings.retrieval_strategy

        # Multi-query expansion
        queries = [query]
        if self._expander is not None:
            queries = self._expander.expand(query)

        # Embed all query variants
        query_embeddings = self._embedder.embed_texts(
            queries, use_cache=False, show_progress=False
        )

        if strat == RetrievalStrategy.DENSE:
            results = self._dense_retrieve(
                queries, query_embeddings, k, metadata_filter
            )
        elif strat == RetrievalStrategy.SPARSE:
            results = self._sparse_retrieve(queries, k, metadata_filter)
        elif strat == RetrievalStrategy.MMR:
            results = self._mmr_retrieve(
                queries, query_embeddings, k, metadata_filter
            )
        elif strat == RetrievalStrategy.ENSEMBLE:
            results = self._ensemble_retrieve(
                queries, query_embeddings, k, metadata_filter
            )
        else:
            logger.warning("Unknown strategy '%s'; using ensemble.", strat)
            results = self._ensemble_retrieve(
                queries, query_embeddings, k, metadata_filter
            )

        # Optional post-retrieval reranking
        if self._compressor is not None:
            results = self._compressor.rerank(query, results)

        return RetrievalResult(
            results=results,
            query=query,
            expanded_queries=queries[1:],
            strategy=strat.value if isinstance(strat, RetrievalStrategy) else str(strat),
        )

    # -- Strategy implementations -------------------------------------------

    def _dense_retrieve(
        self,
        queries: list[str],
        embeddings: np.ndarray,
        top_k: int,
        metadata_filter: Optional[dict[str, Any]],
    ) -> list[SearchResult]:
        """Run dense similarity search for each query variant and deduplicate."""
        seen_ids: set[str] = set()
        all_results: list[SearchResult] = []

        for emb in embeddings:
            results = self._store.similarity_search(
                emb, top_k=top_k, metadata_filter=metadata_filter
            )
            for r in results:
                if r.chunk_id not in seen_ids:
                    seen_ids.add(r.chunk_id)
                    all_results.append(r)

        all_results.sort(key=lambda x: x.score, reverse=True)
        return all_results[:top_k]

    def _sparse_retrieve(
        self,
        queries: list[str],
        top_k: int,
        metadata_filter: Optional[dict[str, Any]],
    ) -> list[SearchResult]:
        """Run BM25 sparse search for each query variant and deduplicate."""
        seen_ids: set[str] = set()
        all_results: list[SearchResult] = []

        for q in queries:
            results = self._store.sparse_search(
                q, top_k=top_k, metadata_filter=metadata_filter
            )
            for r in results:
                if r.chunk_id not in seen_ids:
                    seen_ids.add(r.chunk_id)
                    all_results.append(r)

        all_results.sort(key=lambda x: x.score, reverse=True)
        return all_results[:top_k]

    def _mmr_retrieve(
        self,
        queries: list[str],
        embeddings: np.ndarray,
        top_k: int,
        metadata_filter: Optional[dict[str, Any]],
    ) -> list[SearchResult]:
        """Dense retrieval followed by MMR diversification."""
        # Get a larger candidate set
        fetch_k = min(top_k * 3, 50)
        candidates = self._dense_retrieve(
            queries, embeddings, fetch_k, metadata_filter
        )

        if not candidates:
            return candidates

        # Embed the candidate chunks for MMR
        chunk_texts = [c.text for c in candidates]
        chunk_embs = self._embedder.embed_texts(
            chunk_texts, use_cache=True, show_progress=False
        )

        return mmr_diversify(
            query_embedding=embeddings[0],
            results=candidates,
            chunk_embeddings=chunk_embs,
            top_k=top_k,
            lambda_param=self._settings.mmr_diversity_score,
        )

    def _ensemble_retrieve(
        self,
        queries: list[str],
        embeddings: np.ndarray,
        top_k: int,
        metadata_filter: Optional[dict[str, Any]],
    ) -> list[SearchResult]:
        """Fuse dense and sparse results via reciprocal rank fusion."""
        # Use the hybrid search method from the vector store
        return self._store.hybrid_search(
            query=queries[0],
            query_embedding=embeddings[0],
            top_k=top_k,
            dense_weight=self._settings.dense_weight,
            sparse_weight=self._settings.sparse_weight,
            metadata_filter=metadata_filter,
        )
